# 测试要点

## 社区测试kubernetes性能的关键点

1. 当集群资源使用率是X%（50%、90% 、99%等不同规模下）的时候，创建新的pod所需的时间（这种场景需要提前铺底，然后在铺底基础上用不同的并发梯度创建pod，测试pod创建耗时，评估集群性能）。在测试kubernetes新版本时，一般是以老版本稳定水位（node、pod等）铺底，然后梯度增加进行测试。
2. 当集群使用率高于90%时，容器启动时延的增大（系统会经历一个异常的减速）还有etcd测试的线性性质和“模型建立”的因素。调优方法是：调研etcd新版本是否有解决该问题。
3. 测试的过程中要找出集群的一个最高点，低于和高于这个阈值点，集群性能都不是最优的。
4. 组件负载会消耗master节点的资源，资源消耗所产生的不稳定性和性能问题，会导致集群不可用。所以，在测试过程中要时刻关注资源情况。
5. 客户端创建资源对象的格式 —— API服务对编码和解码JSON对象也需要花费大量的时间 —— 这也可以作为一个优化点。

## 网易容器服务k8s集群性能测试关键点总结

### 集群整体

1. 不同的集群使用水位线（0%，50%， 90%）上，pod/deployment(rs 等资源)创建、扩缩容等核心操作的性能。可以通过预先创建出一批dp（副本数默认设置为3）来填充集群，达到预期的水位，即铺底。
2. 不同水位对系统性能的影响——安全水位，极限水位
3. 容器有无挂载数据盘对容器创建性能的影响。例如，挂载数据盘增加了kubelet挂载磁盘的耗时，会增加pod的启动时长。

### 系统的稳定性

- 系统性能表现，在较长时间范围内的变化趋势
- 系统资源使用情况，在较长时间范围内的变化趋势
- 各个服务组件的TPS、响应时间、错误率
- 内部模块间访问次数、耗时、错误率等内部性能数据
- 各个模块资源使用情况
- 各个服务端组件长时间运行时，是否出现进程意外退出、重启等情况
- 服务端日志是否有未知错误
- 
- 系统日志是否报错。

### apiserver

1. 关注api的响应时间。数据写到etcd即可，然后根据情况关注异步操作是否真正执行完成。
2. 关注apiserver缓存的存储设备对性能的影响。例如，master端节点的磁盘io。
3. 流控对系统、系统性能的影响。
4. apiserver 日志中的错误响应码。
5. apiserver 重启恢复的时间。需要考虑该时间用户是否可接受，重启后请求或者资源使用是否有异常。
6. 关注apiserver在压力测试情况下，响应时间和资源使用情况。

### scheduler

1. 压测scheduler处理能力

- 并发创建大量pod，测试各个pod被调度器调度的耗时（从Pod创建到其被bind到host）
- 不断加大新建的pod数量来增加调度器的负载
- 关注不同pod数量级下，调度器的平均耗时、最大时间、最大QPS（吞吐量）

2. scheduler 重启恢复的时间（从重启开始到重启后系统恢复稳定）。需要考虑该时间用户是否可接受，重启后请求或者资源使用是否有异常。

3. 关注scheduler日志中的错误信息。

### controller

1. 压测 deployment controller处理能力

- 并发创建大量deployment，测试各个deployment被空感知并创建对应rs的耗时
- 观察rs controller创建对应pod的耗时
- 扩容、缩容（缩容到0副本）的耗时
- 不断加大新建deployment的数，测试在不同deployment数量级下，控制器处理deployment的平均耗时、最大时间、最大QPS（吞吐量）和控制器负载等情况

2. controller 重启恢复的时间（从重启开始到重启后系统恢复稳定）。需要考虑该时间用户是否可接受，重启后请求或者资源使用是否有异常。

3. 关注controller日志中的错误信息。



### kubelet

1. 关注etcd 的写入性能

- 写最大并发数
- 写入性能瓶颈，这个主要是定期持久化snapshot操作的性能

2. etcd 的存储设备对性能的影响。例如，写etcd的io。

3. watcher hub 数对k8s系统性能的影响。



# 参考资料

https://www.sohu.com/a/277370414_100298761